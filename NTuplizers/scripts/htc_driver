#!/usr/bin/env python
"""baj_driver.py -- creates scripts for batch-submission of cmsRun jobs
"""
from __future__ import print_function
import argparse
import os
import sys
import math
import json
import datetime

from JMETriggerAnalysis.NTuplizers.utils.common import *

def load_dataset_data(das_name, max_files=-1, max_events=-1, verbose=False):

    dset_data = []

    dataset_split = das_name.split('/')
    if len(dataset_split) != 4:
       KILL(log_prx+'invalid data-set name (format is incorrect, check slashes): '+das_name)

    if 'MINIAOD' not in dataset_split[3]:
       KILL(log_prx+'the data set\'s Tier is not MINIAOD*: '+das_name)

    dataset_files = command_output_lines('dasgoclient --query "file dataset='+str(das_name)+'"')
    dataset_files = [_tmp for _tmp in dataset_files if _tmp != '']
    dataset_files = sorted(list(set(dataset_files)))

    if len(dataset_files) == 0:
       KILL(log_prx+'empty list of input files for dataset: '+str(das_name))

    if max_files > 0:
       dataset_files = dataset_files[:max_files]

    tot_events, break_loop = 0, False

    for i_file in dataset_files:
        i_file_nevents = command_output_lines('dasgoclient --query "file='+str(i_file)+' | grep file.nevents"')
        i_file_nevents = [_tmp.replace(' ', '') for _tmp in i_file_nevents]
        i_file_nevents = [_tmp for _tmp in i_file_nevents if _tmp != '']
        i_file_nevents = sorted(list(set(i_file_nevents)))

        if len(i_file_nevents) != 1:
           KILL('AAA')

        i_file_nevents = i_file_nevents[0]

        if not is_int(i_file_nevents):
           KILL('BBB')

        i_file_nevents = int(i_file_nevents)

        tot_events += i_file_nevents
        if (max_events > 0) and (tot_events >= max_events):
           i_file_nevents -= (tot_events - max_events)
           break_loop = True

        if verbose:
           print(i_file)
           print(i_file_nevents)

        i_file_rawparents = []

        i_file_aodfiles = command_output_lines('dasgoclient --query "parent file='+str(i_file)+'"')
        i_file_aodfiles = [_tmp for _tmp in i_file_aodfiles if _tmp != '']
        i_file_aodfiles = sorted(list(set(i_file_aodfiles)))
        for i_file_aodf in i_file_aodfiles:
            i_file_rawfiles_tmp = command_output_lines('dasgoclient --query "parent file='+str(i_file_aodf)+'"')
            i_file_rawfiles_tmp = [_tmp.replace(' ', '') for _tmp in i_file_rawfiles_tmp]
            i_file_rawfiles_tmp = [_tmp for _tmp in i_file_rawfiles_tmp if _tmp != '']
            i_file_rawfiles_tmp = sorted(list(set(i_file_rawfiles_tmp)))
            i_file_rawparents += i_file_rawfiles_tmp

        i_file_rawparents = sorted(list(set(i_file_rawparents)))

        if verbose:
           for _tmp in i_file_rawparents:
               print(' '*5, _tmp)

        dset_data += [{
          'filesMINIAOD': [i_file],
          'filesMINIAOD.nevents': i_file_nevents,
          'filesRAW': i_file_rawparents,
        }]

        if break_loop:
           break

    del tot_events, break_loop

    return dset_data

def skim_dataset_json(file_path, max_files=-1, max_events=-1, verbose=False):

    dset_data = json.load(open(file_path))

    # checks on .json file content
    if not isinstance(dset_data, list):
       KILL(log_prx+'invalid content of dataset .json file [-d]: '+str(file_path))

    for i_ent in dset_data:

        if not isinstance(i_ent, dict):
           KILL(log_prx+'bbb')

        if 'filesMINIAOD.nevents' not in i_ent:
           KILL(log_prx+'ccc')

        if not isinstance(i_ent['filesMINIAOD.nevents'], int):
           KILL(log_prx+'ddd')

        for _tmp in ['filesMINIAOD', 'filesRAW']:

            if _tmp not in i_ent:
               KILL(log_prx+'ccc '+_tmp)

            if not isinstance(i_ent[_tmp], list):
               KILL(log_prx+'ddd '+_tmp)

            for _tmp2 in i_ent[_tmp]:
                if not isinstance(_tmp2, str):
                   KILL(log_prx+'eee '+_tmp)

    # skim input list based on max_files and max_events
    if max_files > 0:
       dset_data = dset_data[max_files]

    if max_events > 0:
       lastIndex, totEvents = 0, 0
       for _tmp in dset_data:
           lastIndex += 1
           i_file_nevents = _tmp['filesMINIAOD.nevents']
           totEvents += i_file_nevents
           if totEvents >= max_events:
              i_file_nevents -= (totEvents - max_events)
              _tmp['filesMINIAOD.nevents'] = i_file_nevents
              break
       if lastIndex != len(dset_data):
          dset_data = dset_data[:lastIndex]
       del lastIndex, totEvents

    return dset_data

#### main
if __name__ == '__main__':
    ### args
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument('-c', '--cfg', dest='cmsRun_cfg', action='store', default=None, required=True,
                        help='path to cmsRun cfg file')

    parser.add_argument('--customize-cfg', dest='customize_cfg', action='store_true', default=False,
                        help='append minimal customization to cmsRun-cfg required by the driver')

    parser.add_argument('-d', '--dataset', dest='dataset', action='store', default=None, required=True,
                        help='name of MINIAOD* input data set in DAS')

    parser.add_argument('-o', '--output', dest='output', action='store', default=None, required=True,
                        help='path to output directory')

    parser.add_argument('-j', '--jobdirname', dest='jobdirname', action='store', default='job', required=False,
                        help='prefix of batch-job sub-directories (example: [JOBDIRNAME]_[counter]/)')

    parser.add_argument('-f', '--max-files', dest='max_files', action='store', type=int, default=-1, required=False,
                        help='maximum number of input files to be processed (if integer is negative, all files will be processed)')

    parser.add_argument('-m', '--max-events', dest='max_events', action='store', type=int, default=-1, required=False,
                        help='maximum number of total input events to be processed (if integer is negative, all events will be processed)')

    parser.add_argument('-n', '--n-events', dest='n_events', action='store', type=int, default=-1, required=False,
                        help='maximum number of events per job')

    parser.add_argument('--cpus', dest='cpus', action='store', type=int, default=1, required=False,
                        help='argument of HTCondor parameter "RequestCpus"')

    parser.add_argument('--memory', dest='memory', action='store', type=int, default=2000, required=False,
                        help='argument of HTCondor parameter "RequestMemory"')

    parser.add_argument('--runtime', dest='runtime', action='store', type=int, default=10800, required=False,
                        help='argument of HTCondor parameter "+RequestRuntime"')

    parser.add_argument('--batch-name', dest='batch_name', action='store', default=None, required=False,
                        help='argument of HTCondor parameter "batch_name" (if not specified, basename of argument of "--output" is used)')

    parser.add_argument('--submit', dest='submit', action='store_true', default=False,
                        help='submit job(s) on the batch system')

    parser.add_argument('--no-export-LD-LIBRARY-PATH', dest='no_export_LD_LIBRARY_PATH', action='store_true', default=False,
                        help='do not export explicitly the environment variable "LD_LIBRARY_PATH" in the batch-job executable')

    parser.add_argument('--dry-run', dest='dry_run', action='store_true', default=False,
                        help='enable dry-run mode')

    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,
                        help='enable verbose mode')

    opts, opts_unknown = parser.parse_known_args()
    ### -------------------------

    log_prx = os.path.basename(__file__)+' -- '

    ### opts --------------------
    VERBOSE = bool(opts.verbose and (not opts.submit))

    if 'CMSSW_BASE' not in os.environ:
       KILL(log_prx+'global variable CMSSW_BASE is not defined (set up CMSSW environment with "cmsenv" before submitting jobs)')

    if not os.path.isfile(opts.cmsRun_cfg):
       KILL(log_prx+'invalid path to cmsRun cfg file [-c]: '+str(opts.cmsRun_cfg))

    if os.path.exists(opts.output):
       KILL(log_prx+'target path to output directory already exists [-o]: '+str(opts.output))

    OUTPUT_DIR = os.path.abspath(opts.output)

    if opts.n_events == 0:
       KILL(log_prx+'logic error: requesting zero events per job (use non-zero value for argument of option "-n")')

    if opts.submit and (not opts.dry_run):
       which('condor_submit')

    if opts.cpus <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "RequestCpus": '+str(opts.cpus))

    if opts.memory <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "RequestMemory": '+str(opts.memory))

    if opts.runtime <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "+RequestRuntime": '+str(opts.runtime))

    is_slc7_arch = False
    if os.environ['SCRAM_ARCH'].startswith('slc7'): is_slc7_arch = True
    elif os.environ['SCRAM_ARCH'].startswith('slc6'): pass
    else:
       KILL(log_prc+'could not infer architecture from environment variable "SCRAM_ARCH": '+str(os.environ['SCRAM_ARCH']))

    if opts.max_events == 0:
       KILL(log_prx+'logic error: requesting a maximum of zero input events (use non-zero value for argument of option --max-events/-m)')

    if opts.max_files == 0:
       KILL(log_prx+'logic error: requesting a maximum of zero input files (use non-zero value for argument of option --max-files/-f)')

    ### unrecognized command-line arguments
    ### -> used as additional command-line arguments to cmsRun
    if len(opts_unknown):
       print('-'*50)
       print(colored_text('additional cmsRun command-line arguments:', ['1']))
       for _tmp in opts_unknown: print(' '+str(_tmp))
       print('-'*50)

    ### extract input-files information from data set name via dasgoclient
    ### -> list of dictionaries, each containing names of MINIAOD files, RAW parent files, and number of events per file
    inputfileDicts = []

    if os.path.isfile(opts.dataset):
       inputfileDicts = skim_dataset_json(file_path=opts.dataset, max_files=opts.max_files, max_events=opts.max_events, verbose=opts.verbose)
    else:
       inputfileDicts = load_dataset_data(das_name=opts.dataset, max_files=opts.max_files, max_events=opts.max_events, verbose=opts.verbose)

    ### total number of events and jobs
    ### -> used to determine format of output-file name
    njobs = 0
    for i_inpfdc in inputfileDicts:
        njobs += int(math.ceil(float(i_inpfdc['filesMINIAOD.nevents']) / opts.n_events)) if (opts.n_events > 0) else 1
    if njobs == 0:
       KILL(log_prx+'input error: expected number of batch jobs is zero (check input data set and number of events): '+opts.dataset)
    outputname_postfix_format = '_{:0'+str(1+int(math.log10(njobs)))+'d}'
    del njobs

    ### grid certificate
    voms_cert_path = None

    if ('X509_USER_PROXY' in os.environ):
       voms_cert_path = os.environ['X509_USER_PROXY']
    else:
       voms_cert_path = '/tmp/x509up_u'+str(os.getuid())

    if not os.path.isfile(voms_cert_path):
       EXE('voms-proxy-init --voms cms', verbose=opts.verbose, dry_run=opts.dry_run)

    if not os.path.isfile(voms_cert_path):
       KILL(log_prx+'invalid path to voms certificate: '+voms_cert_path)

    EXE('mkdir -p '+OUTPUT_DIR, verbose=opts.verbose, dry_run=opts.dry_run)
    EXE('cp '+voms_cert_path+' '+OUTPUT_DIR+'/X509_USER_PROXY', verbose=opts.verbose, dry_run=opts.dry_run)

    ### copy cmsRun-cfg into output directory
    out_cmsRun_cfg = os.path.abspath(OUTPUT_DIR+'/cfg.py')
    EXE('cp '+opts.cmsRun_cfg+' '+out_cmsRun_cfg, verbose=opts.verbose, dry_run=opts.dry_run)

    if opts.customize_cfg:
       with open(out_cmsRun_cfg, 'a') as cfg_file:
            custom_str = """
###
### customization added by {:} [time-stamp: {:}]
###

import FWCore.ParameterSet.VarParsing as vpo
opts = vpo.VarParsing('analysis')

opts.register('skipEvents', 0,
              vpo.VarParsing.multiplicity.singleton,
              vpo.VarParsing.varType.int,
              'number of events to be skipped')

opts.register('dumpPython', None,
              vpo.VarParsing.multiplicity.singleton,
              vpo.VarParsing.varType.string,
              'Path to python file with content of cms.Process')

# max number of events to be processed
process.maxEvents.input = opts.maxEvents

# number of events to be skipped
process.source.skipEvents = cms.untracked.uint32(opts.skipEvents)

# input EDM files [primary]
process.source.fileNames = opts.inputFiles

# input EDM files [secondary]
process.source.secondaryFileNames = opts.secondaryInputFiles

# dump content of cms.Process to python file
if opts.dumpPython is not None:
   open(opts.dumpPython, 'w').write(process.dumpPython())
"""
            cfg_file.write(custom_str.format(os.path.basename(__file__), str(datetime.datetime.now())))

    ### copy driver command
    with open(os.path.abspath(OUTPUT_DIR+'/cmdLog'), 'w') as file_cmdLog:
         file_cmdLog.write((' '.join(sys.argv[:]))+'\n')

    ### condor submission file
    SUBFILE_ABSPATH = OUTPUT_DIR+'/condor.sub'

    if os.path.exists(SUBFILE_ABSPATH):
       KILL(log_prx+'target output file already exists (condor submission file): '+SUBFILE_ABSPATH)

    SUBFILE_LINES = [
      'BASEDIR = '+OUTPUT_DIR,
      'batch_name = '+(opts.batch_name if (opts.batch_name is not None) else '$Fn(BASEDIR)'),
      'initialdir = $DIRNAME(QFILE)',
      'executable = $(initialdir)/exe.sh',
      'output = logs/out.$(Cluster).$(Process)',
      'error  = logs/err.$(Cluster).$(Process)',
      'log    = logs/log.$(Cluster).$(Process)',
      '#arguments =',
      '#transfer_executable = True',
      '#transfer_input_files =',
      'universe = vanilla',
      'getenv = True',
      'should_transfer_files = IF_NEEDED',
      'when_to_transfer_output = ON_EXIT',
      'requirements = (OpSysAndVer == "'+('CentOS7' if is_slc7_arch else 'SL6')+'")',
      'RequestCpus = '+str(opts.cpus),
      'RequestMemory = '+str(opts.memory),
      '+RequestRuntime = '+str(opts.runtime),
      'x509userproxy = $(BASEDIR)/X509_USER_PROXY',
      'queue QFILE matching files $(BASEDIR)/*/flag.queue',
    ]

    with open(SUBFILE_ABSPATH, 'w') as f_subfile:
       for _tmp in SUBFILE_LINES:
           f_subfile.write(_tmp+'\n')

    ### create output batch scripts
    job_counter = 0

    for i_inpfdc in inputfileDicts:

        i_inputFiles = i_inpfdc['filesMINIAOD']
        i_inputFiles_nevents = i_inpfdc['filesMINIAOD.nevents']
        i_secondaryInputFiles = i_inpfdc['filesRAW']

        # number of jobs for this set of input files
        i_njobs = int(math.ceil(float(i_inputFiles_nevents) / opts.n_events)) if (opts.n_events > 0) else 1
        i_nevt_remainder = i_inputFiles_nevents%opts.n_events if (opts.n_events > 0) else 0

        for i_job in range(i_njobs):

            # name of output sub-directory
            i_OUTPUT_DIR = OUTPUT_DIR+'/'+opts.jobdirname+outputname_postfix_format.format(job_counter)

            # create logs/ directory for HTCondor log-files
            EXE('mkdir -p '+str(i_OUTPUT_DIR)+'/logs', verbose=opts.verbose, dry_run=opts.dry_run)

            # create (empty) queue-file to trigger job submission
            EXE('touch '+str(i_OUTPUT_DIR)+'/flag.queue', verbose=opts.verbose, dry_run=opts.dry_run)

            # number of events for this job
            i_maxEvents = min(opts.n_events, i_inputFiles_nevents) if (opts.n_events > 0) else i_inputFiles_nevents
            if (i_job == (i_njobs - 1)) and (i_nevt_remainder != 0):
               i_maxEvents = i_nevt_remainder

            # cmsRun arguments (cfg-file + options)
            cmsRun_opts = out_cmsRun_cfg
            cmsRun_opts += ' \\\n maxEvents='+str(i_maxEvents)
            cmsRun_opts += ' \\\n skipEvents='+str(opts.n_events*i_job)
            cmsRun_opts += ' \\\n inputFiles='+str(','.join(i_inputFiles))
            cmsRun_opts += ' \\\n secondaryInputFiles='+str(','.join(i_secondaryInputFiles))
            for _tmp in opts_unknown:
                cmsRun_opts += ' \\\n '+str(_tmp)

            ## dump of full python config
            if job_counter == 0:
               EXE('python '+cmsRun_opts+' dumpPython='+OUTPUT_DIR+'/dumpPython.py &> /dev/null', suspend=False, verbose=opts.verbose, dry_run=opts.dry_run)

            i_SHELL_COMMANDS  = [['set -e']]
            i_SHELL_COMMANDS += [['if [ -f flag.done ]; then exit 0; fi;']]
            i_SHELL_COMMANDS += [['cd '+os.environ['CMSSW_BASE']+'/src']]
            i_SHELL_COMMANDS += [['eval `scram runtime -sh`']]
            i_SHELL_COMMANDS += [['cd -']]
            i_SHELL_COMMANDS += [['cmsRun '+cmsRun_opts]]
            i_SHELL_COMMANDS += [['touch flag.done']]

            EXEFILE_ABSPATH = i_OUTPUT_DIR+'/exe.sh'

            if os.path.exists(EXEFILE_ABSPATH):
               KILL(log_prx+'target output file already exists (executable of batch job): '+EXEFILE_ABSPATH)

            with open(EXEFILE_ABSPATH, 'w') as f_exefile:

               f_exefile.write('#!/bin/bash\n')

               # export explicitly the environment variable LD_LIBRARY_PATH
               if not opts.no_export_LD_LIBRARY_PATH:
                  if 'LD_LIBRARY_PATH' in os.environ:
                     f_exefile.write('\n'+'export LD_LIBRARY_PATH='+os.environ['LD_LIBRARY_PATH']+'\n')

               f_exefile.write('\n'+('\n\n'.join([' \\\n '.join(_tmp) for _tmp in i_SHELL_COMMANDS]))+'\n')

            print(colored_text('output:', ['1', '94']), os.path.relpath(EXEFILE_ABSPATH, os.environ['PWD']))

            EXE('chmod u+x '+EXEFILE_ABSPATH, verbose=opts.verbose, dry_run=opts.dry_run)

            job_counter += 1

    if opts.submit:
       EXE('condor_submit '+SUBFILE_ABSPATH, suspend=False, verbose=opts.verbose, dry_run=opts.dry_run)
